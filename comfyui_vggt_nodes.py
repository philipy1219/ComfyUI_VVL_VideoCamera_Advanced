import os
import sys
sys.path.append(
    os.path.dirname(os.path.abspath(__file__))
)
import json
import tempfile
from typing import List, Any, Dict
import logging
import math

import cv2
import numpy as np
import torch
from PIL import Image
import torch.nn.functional as F

# å°è¯•å¯¼å…¥ ComfyUI çš„ç±»å‹æ ‡è®°
try:
    from comfy.comfy_types import IO
except ImportError:
    class IO:
        VIDEO = "VIDEO"
        IMAGE = "IMAGE"

# å¯¼å…¥ VGGT ç›¸å…³å‡½æ•°
try:
    from vggt.utils.load_fn import load_and_preprocess_images
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    VGGT_UTILS_AVAILABLE = True
except Exception as e:
    load_and_preprocess_images = None
    pose_encoding_to_extri_intri = None
    VGGT_UTILS_AVAILABLE = False
    _VGGT_UTILS_IMPORT_ERROR = e

# å¯¼å…¥æ¨¡å‹åŠ è½½å™¨
try:
    from .vggt_model_loader import VVLVGGTLoader
    MODEL_LOADER_AVAILABLE = True
except ImportError:
    VVLVGGTLoader = None
    MODEL_LOADER_AVAILABLE = False

# é…ç½®æ—¥å¿—
logger = logging.getLogger('vvl_vggt_nodes')

# -----------------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# -----------------------------------------------------------------------------

def _extract_video_frames(video_path: str, interval: int, max_frames: int) -> List[np.ndarray]:
    """æŒ‰ç…§ç»™å®šé—´éš”æå–è§†é¢‘å¸§ (BGR)."""
    cap = cv2.VideoCapture(video_path)
    frames = []
    idx = 0
    while len(frames) < max_frames:
        ret, frame = cap.read()
        if not ret:
            break
        if idx % interval == 0:
            frames.append(frame.copy())
        idx += 1
    cap.release()
    return frames

def _matrices_to_json(intrinsic, extrinsic) -> (str, str):
    """å°†ç›¸æœºçŸ©é˜µè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ã€‚"""
    num_views = extrinsic.shape[0]
    intrinsics_list = []
    poses_list = []
    for i in range(num_views):
        K = intrinsic[i].tolist()
        Rt = extrinsic[i].tolist()
        # ç›¸æœºä½ç½® world åæ ‡ ( -R^T * t )
        R = np.array(Rt)[:3, :3]
        t = np.array(Rt)[:3, 3]
        position = (-R.T @ t).tolist()
        intrinsics_list.append({
            "view_id": i,
            "intrinsic_matrix": K
        })
        poses_list.append({
            "view_id": i,
            "extrinsic_matrix": Rt,
            "position": position
        })
    return (
        json.dumps({"cameras": intrinsics_list}, ensure_ascii=False, indent=2),
        json.dumps({"poses": poses_list}, ensure_ascii=False, indent=2),
    )

def _create_traj_preview(extrinsic: torch.Tensor) -> torch.Tensor:
    """æ ¹æ®ç›¸æœºå¤–å‚åˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼ˆä½¿ç”¨matplotlib 3Dç»˜å›¾ï¼‰ã€‚"""
    ext = extrinsic.cpu().numpy()  # (N,3,4)
    positions = []
    orientations = []
    
    for mat in ext:
        R = mat[:3, :3]
        t = mat[:3, 3]
        pos = -R.T @ t  # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®
        positions.append(pos)
        # æå–ç›¸æœºæœå‘ï¼ˆZè½´æ–¹å‘ï¼‰
        forward = -R[:, 2]  # ç›¸æœºæœå‘ï¼ˆZè½´è´Ÿæ–¹å‘ï¼‰
        orientations.append(forward)
    
    positions = np.array(positions)
    orientations = np.array(orientations)
    
    # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
    if len(positions) == 0:
        print("VGGT: æ²¡æœ‰æœ‰æ•ˆçš„ç›¸æœºä½å§¿æ•°æ®")
        return _create_insufficient_data_image()
    
    print(f"VGGT: å¤„ç† {len(positions)} ä¸ªç›¸æœºä½å§¿")
    
    # å³ä½¿åªæœ‰ä¸€ä¸ªä½å§¿ä¹Ÿå¯ä»¥æ˜¾ç¤º
    if len(positions) == 1:
        print("VGGT: å•ä¸ªç›¸æœºä½å§¿ï¼Œå°†åˆ›å»ºç®€åŒ–å¯è§†åŒ–")

    try:
        # ä½¿ç”¨matplotlibåˆ›å»º3Dç«‹ä½“å¯è§†åŒ–
        import matplotlib.pyplot as plt
        from mpl_toolkits.mplot3d import Axes3D
        
        fig = plt.figure(figsize=(12, 9))
        ax = fig.add_subplot(111, projection='3d')
        
        # ç»˜åˆ¶è½¨è¿¹çº¿
        if len(positions) > 1:
            ax.plot(positions[:, 0], positions[:, 1], positions[:, 2], 
                   'b-', linewidth=3, alpha=0.8, label='Camera Path')
        
        # ç”¨é¢œè‰²æ¸å˜è¡¨ç¤ºæ—¶é—´è¿›ç¨‹
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                           c=colors, s=60, alpha=0.9, edgecolors='black', linewidth=0.5)
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        if len(positions) > 0:
            ax.scatter([positions[0, 0]], [positions[0, 1]], [positions[0, 2]], 
                      c='green', s=150, marker='^', label='Start', edgecolors='darkgreen', linewidth=2)
        if len(positions) > 1:
            ax.scatter([positions[-1, 0]], [positions[-1, 1]], [positions[-1, 2]], 
                      c='red', s=150, marker='o', label='End', edgecolors='darkred', linewidth=2)
        
        # æ·»åŠ ç›¸æœºæ–¹å‘æŒ‡ç¤ºå™¨ï¼ˆæ¯å‡ ä¸ªä½å§¿æ˜¾ç¤ºä¸€ä¸ªï¼‰
        if len(positions) > 0 and len(orientations) > 0:
            # å®‰å…¨åœ°è®¡ç®—æ˜¾ç¤ºæ­¥é•¿ï¼Œæœ€å¤šæ˜¾ç¤º10ä¸ªç®­å¤´
            step = max(1, len(positions) // 10)
            
            # è®¡ç®—åˆé€‚çš„ç®­å¤´é•¿åº¦
            position_range = positions.max(axis=0) - positions.min(axis=0)
            scene_scale = np.linalg.norm(position_range)
            
            # æ›´æ˜æ˜¾çš„ç®­å¤´é•¿åº¦è®¡ç®—ï¼Œç¡®ä¿ç®­å¤´æ¸…æ™°å¯è§
            if scene_scale < 1e-6:
                direction_length = 0.5  # æå°åœºæ™¯ä½¿ç”¨æ›´å¤§çš„é»˜è®¤é•¿åº¦
            else:
                # ä½¿ç”¨æ›´æ˜æ˜¾çš„æ¯”ä¾‹ï¼š8%~20%
                direction_length = scene_scale * 0.15  # åŸºå‡† 15%
                min_len = scene_scale * 0.08  # æœ€å°8%
                max_len = scene_scale * 0.20   # æœ€å¤§20%
                direction_length = max(min_len, min(direction_length, max_len))
                
                # è¿›ä¸€æ­¥é™åˆ¶æœ€å¤§é•¿åº¦ï¼Œé¿å…ç®­å¤´è¿‡é•¿
                absolute_max_length = scene_scale * 0.4  # ç»å¯¹ä¸è¶…è¿‡åœºæ™¯å°ºåº¦çš„40%
                direction_length = min(direction_length, absolute_max_length)
            
            # ç»˜åˆ¶ç®­å¤´ï¼Œç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            arrow_count = 0
            for i in range(0, len(positions), step):
                if i < len(orientations) and arrow_count < 12:  # å‡å°‘åˆ°æœ€å¤š12ä¸ªç®­å¤´
                    pos = positions[i]
                    direction = orientations[i]
                    
                    # å½’ä¸€åŒ–æ–¹å‘å‘é‡ï¼Œé¿å…å¼‚å¸¸é•¿åº¦
                    direction_norm = np.linalg.norm(direction)
                    if direction_norm > 1e-6:
                        direction = direction / direction_norm
                        direction_scaled = direction * direction_length
                        
                        # æ£€æŸ¥ç®­å¤´ç»ˆç‚¹æ˜¯å¦ä¼šè¶…å‡ºåœºæ™¯è¾¹ç•Œ
                        arrow_end = pos + direction_scaled
                        scene_min = positions.min(axis=0)
                        scene_max = positions.max(axis=0)
                        
                        # å¦‚æœç®­å¤´ä¼šè¶…å‡ºè¾¹ç•Œï¼Œè¿›ä¸€æ­¥ç¼©çŸ­
                        for axis in range(3):
                            if arrow_end[axis] < scene_min[axis] or arrow_end[axis] > scene_max[axis]:
                                direction_scaled *= 0.7  # ç¼©çŸ­30%
                                break
                        
                        ax.quiver(pos[0], pos[1], pos[2], 
                                 direction_scaled[0], direction_scaled[1], direction_scaled[2], 
                                 color='orange', alpha=0.8, arrow_length_ratio=0.2, linewidth=1.5)
                        arrow_count += 1
        
        # è®¾ç½®åæ ‡è½´
        ax.set_xlabel('X (meters)', fontsize=12)
        ax.set_ylabel('Y (meters)', fontsize=12)
        ax.set_zlabel('Z (meters)', fontsize=12)
        ax.set_title('VGGT Camera Trajectory (3D View)', fontsize=14, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹
        ax.legend(loc='upper right', fontsize=10)
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20)
        cbar.set_label('Time Progress', fontsize=10)
        
        # è®¾ç½®ç›¸ç­‰çš„åæ ‡è½´æ¯”ä¾‹
        if len(positions) > 1:
            max_range = np.array([positions.max(axis=0) - positions.min(axis=0)]).max() / 2.0
            mid_x = (positions.max(axis=0)[0] + positions.min(axis=0)[0]) * 0.5
            mid_y = (positions.max(axis=0)[1] + positions.min(axis=0)[1]) * 0.5
            mid_z = (positions.max(axis=0)[2] + positions.min(axis=0)[2]) * 0.5
        else:
            # å•ä¸ªä½å§¿çš„æƒ…å†µï¼Œè®¾ç½®ä¸€ä¸ªåˆç†çš„æ˜¾ç¤ºèŒƒå›´
            max_range = 2.0  # é»˜è®¤2ç±³çš„æ˜¾ç¤ºèŒƒå›´
            mid_x, mid_y, mid_z = positions[0]
        
        # ç¡®ä¿èŒƒå›´ä¸ä¸ºé›¶
        if max_range < 0.1:
            max_range = 1.0
        
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        
        # è®¾ç½®ç½‘æ ¼
        ax.grid(True, alpha=0.3)
        
        # è°ƒæ•´è§†è§’
        ax.view_init(elev=20, azim=45)
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                print(f"VGGT: æˆåŠŸåˆ›å»º3Dè½¨è¿¹å¯è§†åŒ–ï¼Œå›¾åƒå°ºå¯¸: {img.shape}")
                return img_tensor.unsqueeze(0)
            else:
                print("VGGT: è¯»å–ç”Ÿæˆçš„å¯è§†åŒ–å›¾åƒå¤±è´¥")
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»º3Då¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœ3Då¯è§†åŒ–å¤±è´¥ï¼Œåˆ›å»ºå¤‡ç”¨çš„2Då¯è§†åŒ–
        return _create_fallback_2d_visualization_vggt(positions, orientations)

def _create_fallback_2d_visualization_vggt(positions: np.ndarray, orientations: np.ndarray) -> torch.Tensor:
    """åˆ›å»ºVGGTå¤‡ç”¨2Då¯è§†åŒ–ï¼ˆå½“3Då¯è§†åŒ–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        import matplotlib.pyplot as plt
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('VGGT Camera Trajectory (2D Views)', fontsize=16, fontweight='bold')
        
        # XYè§†å›¾ï¼ˆä¿¯è§†å›¾ï¼‰
        if len(positions) > 1:
            ax1.plot(positions[:, 0], positions[:, 1], 'b-', linewidth=2, alpha=0.7)
        colors = plt.cm.viridis(np.linspace(0, 1, len(positions)))
        ax1.scatter(positions[:, 0], positions[:, 1], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax1.scatter(positions[0, 0], positions[0, 1], c='green', s=100, marker='^', label='Start')
        if len(positions) > 1:
            ax1.scatter(positions[-1, 0], positions[-1, 1], c='red', s=100, marker='o', label='End')
        ax1.set_xlabel('X (meters)')
        ax1.set_ylabel('Y (meters)')
        ax1.set_title('Top View (XY)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # XZè§†å›¾ï¼ˆä¾§è§†å›¾ï¼‰
        if len(positions) > 1:
            ax2.plot(positions[:, 0], positions[:, 2], 'g-', linewidth=2, alpha=0.7)
        ax2.scatter(positions[:, 0], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax2.scatter(positions[0, 0], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax2.scatter(positions[-1, 0], positions[-1, 2], c='red', s=100, marker='o')
        ax2.set_xlabel('X (meters)')
        ax2.set_ylabel('Z (meters)')
        ax2.set_title('Side View (XZ)')
        ax2.grid(True, alpha=0.3)
        
        # YZè§†å›¾ï¼ˆæ­£è§†å›¾ï¼‰
        if len(positions) > 1:
            ax3.plot(positions[:, 1], positions[:, 2], 'r-', linewidth=2, alpha=0.7)
        ax3.scatter(positions[:, 1], positions[:, 2], c=colors, s=50, alpha=0.8, edgecolors='black')
        if len(positions) > 0:
            ax3.scatter(positions[0, 1], positions[0, 2], c='green', s=100, marker='^')
        if len(positions) > 1:
            ax3.scatter(positions[-1, 1], positions[-1, 2], c='red', s=100, marker='o')
        ax3.set_xlabel('Y (meters)')
        ax3.set_ylabel('Z (meters)')
        ax3.set_title('Front View (YZ)')
        ax3.grid(True, alpha=0.3)
        
        # ç»Ÿè®¡ä¿¡æ¯é¢æ¿
        ax4.axis('off')
        stats_text = f"""VGGT Trajectory Statistics
        
Total Poses: {len(positions)}
Position Range:
  X: [{positions[:, 0].min():.3f}, {positions[:, 0].max():.3f}]
  Y: [{positions[:, 1].min():.3f}, {positions[:, 1].max():.3f}]
  Z: [{positions[:, 2].min():.3f}, {positions[:, 2].max():.3f}]

Path Length: {np.sum(np.linalg.norm(np.diff(positions, axis=0), axis=1)):.3f}m"""
        
        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸ºå›¾åƒ
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            plt.savefig(tmp.name, dpi=120, bbox_inches='tight', facecolor='white')
            plt.close()
            
            # è¯»å–å›¾åƒ
            img = cv2.imread(tmp.name)
            os.unlink(tmp.name)
            
            if img is not None:
                # BGRè½¬RGB
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºtorch tensor
                img_tensor = torch.from_numpy(img.astype(np.float32) / 255.0)
                return img_tensor.unsqueeze(0)
            else:
                return _create_insufficient_data_image()
    
    except Exception as e:
        print(f"VGGT: åˆ›å»ºå¤‡ç”¨2Då¯è§†åŒ–ä¹Ÿå¤±è´¥: {e}")
        return _create_insufficient_data_image()

def _create_insufficient_data_image():
    """åˆ›å»ºæ•°æ®ä¸è¶³çš„æç¤ºå›¾åƒ"""
    canvas = np.ones((600, 800, 3), dtype=np.float32) * 0.9
    cv2.putText(canvas, "Insufficient Camera Data", (200, 280), 
                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0.3, 0.3, 0.3), 2)
    cv2.putText(canvas, "Need at least 2 frames", (250, 320), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0.5, 0.5, 0.5), 1)
    return torch.from_numpy(canvas).unsqueeze(0)

def preprocess_tensor_like_vggt(imgs, mode="crop"):
    """
    imgs: [B, C, H, W] æˆ– [B, H, W, C]ï¼Œåƒç´ å€¼0~1
    è¿”å›: [B, 3, H, W]ï¼Œå®½518ï¼Œé«˜ä¸º14çš„å€æ•°
    """
    target_size = 518

    # å¦‚æœæ˜¯ [B, H, W, C]ï¼Œè½¬ä¸º [B, C, H, W]
    if imgs.shape[-1] == 3:
        imgs = imgs.permute(0, 3, 1, 2).contiguous()

    B, C, H, W = imgs.shape

    # è®¡ç®—æ–°å°ºå¯¸
    if mode == "pad":
        if W >= H:
            new_W = target_size
            new_H = round(H * (new_W / W) / 14) * 14
        else:
            new_H = target_size
            new_W = round(W * (new_H / H) / 14) * 14
    else:  # crop
        new_W = target_size
        new_H = round(H * (new_W / W) / 14) * 14

    # resize
    imgs = F.interpolate(imgs, size=(new_H, new_W), mode='bilinear', align_corners=False)

    # cropæ¨¡å¼ä¸‹ï¼Œå¦‚æœé«˜åº¦å¤§äº518ï¼Œä¸­å¿ƒè£å‰ª
    if mode == "crop" and new_H > target_size:
        start_y = (new_H - target_size) // 2
        imgs = imgs[:, :, start_y : start_y + target_size, :]

    # padæ¨¡å¼ä¸‹ï¼Œpadåˆ°æ­£æ–¹å½¢
    if mode == "pad":
        h_padding = target_size - imgs.shape[2]
        w_padding = target_size - imgs.shape[3]
        if h_padding > 0 or w_padding > 0:
            pad_top = h_padding // 2
            pad_bottom = h_padding - pad_top
            pad_left = w_padding // 2
            pad_right = w_padding - pad_left
            imgs = F.pad(
                imgs, (pad_left, pad_right, pad_top, pad_bottom), mode="constant", value=1.0
            )

    return imgs

# -----------------------------------------------------------------------------
# ä¸»è¦èŠ‚ç‚¹å®ç°
# -----------------------------------------------------------------------------

class VGGTVideoCameraNode:
    """VGGT è§†é¢‘ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "vggt_model": ("VVL_VGGT_MODEL", {
                    "tooltip": "æ¥è‡ªVVLVGGTLoaderçš„VGGTæ¨¡å‹å®ä¾‹ï¼ŒåŒ…å«å·²åŠ è½½çš„æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯"
                }),
                "video": (IO.VIDEO, {
                    "tooltip": "æ¥è‡ª LoadVideo çš„è§†é¢‘å¯¹è±¡ï¼Œæˆ–ç›´æ¥è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„"
                }),
            },
            "optional": {
                "video_path": ("STRING", {
                    "default": "",
                    "tooltip": "å¤‡ç”¨è§†é¢‘è·¯å¾„ï¼Œå½“videoè¾“å…¥ä¸ºç©ºæ—¶ä½¿ç”¨"
                }),
                "frame_interval": ("INT", {
                    "default": 5, "min": 1, "max": 50, "step": 1,
                    "tooltip": "å¸§æå–é—´éš”ï¼Œæ•°å€¼è¶Šå°æå–çš„å¸§è¶Šå¯†é›†ï¼Œä½†è®¡ç®—é‡æ›´å¤§"
                }),
                "max_frames": ("INT", {
                    "default": 60, "min": 5, "max": 200, "step": 5,
                    "tooltip": "æœ€å¤§æå–å¸§æ•°ï¼Œç”¨äºæ§åˆ¶è®¡ç®—é‡å’Œå†…å­˜ä½¿ç”¨"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_preview", "poses_json")
    FUNCTION = "estimate"
    CATEGORY = "ğŸ’ƒVVL/VGGT"

    # ---------------------------------------------------------
    def _resolve_video_path(self, video: Any, fallback: str) -> str:
        """è§£æè§†é¢‘è·¯å¾„"""
        if video is None:
            return fallback
        # å¦‚æœ video æ˜¯å­—ç¬¦ä¸²
        if isinstance(video, str):
            return video
        # å¸¸è§å±æ€§
        attrs = ["_VideoFromFile__file", "path", "video_path", "_path", "file_path"]
        for attr in attrs:
            if hasattr(video, attr):
                val = getattr(video, attr)
                if isinstance(val, str):
                    return val
        return fallback

    # ---------------------------------------------------------
    def estimate(self, vggt_model: Dict, video=None, video_path: str = "", 
                frame_interval: int = 5, max_frames: int = 60):
        """æ‰§è¡Œç›¸æœºå‚æ•°ä¼°è®¡"""
        try:
            # æ£€æŸ¥VGGTå·¥å…·å‡½æ•°æ˜¯å¦å¯ç”¨
            if not VGGT_UTILS_AVAILABLE:
                raise RuntimeError(f"VGGT utils not available: {_VGGT_UTILS_IMPORT_ERROR}")
            
            # ä»æ¨¡å‹å­—å…¸ä¸­è·å–ä¿¡æ¯
            model_instance = vggt_model['model']
            device = vggt_model['device']
            model_name = vggt_model['model_name']
            
            logger.info(f"VGGTVideoCameraNode: Using {model_name} on {device}")
            
            # ç¡®å®šæ•°æ®ç±»å‹
            if device.type == "cuda":
                try:
                    # å°è¯•ä½¿ç”¨BFloat16ï¼Œå¦‚æœä¸æ”¯æŒåˆ™fallbackåˆ°Float16
                    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
                except:
                    dtype = torch.float16
            else:
                dtype = torch.float32

            # è§£æè§†é¢‘è·¯å¾„
            vid_path = self._resolve_video_path(video, video_path)
            if not vid_path or not os.path.exists(vid_path):
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶: {vid_path}")

            logger.info(f"VGGTVideoCameraNode: Processing video: {vid_path}")

            # æå–è§†é¢‘å¸§
            frames = _extract_video_frames(vid_path, frame_interval, max_frames)
            if not frames:
                raise RuntimeError("æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§")

            logger.info(f"VGGTVideoCameraNode: Extracted {len(frames)} frames")

            # å°†å¸§ä¿å­˜ä¸º PNG ä»¥å¤ç”¨å®˜æ–¹é¢„å¤„ç†
            with tempfile.TemporaryDirectory() as tmpdir:
                img_paths = []
                for i, frm in enumerate(frames):
                    rgb = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)
                    p = os.path.join(tmpdir, f"frame_{i:04d}.png")
                    # ä½¿ç”¨PILä¿å­˜RGBå›¾ç‰‡ï¼Œé¿å…cv2çš„BGRé—®é¢˜
                    Image.fromarray(rgb).save(p)
                    img_paths.append(p)

                # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
                imgs = load_and_preprocess_images(img_paths).to(device)
                logger.info(f"VGGTVideoCameraNode: Preprocessed images shape: {imgs.shape}")

                # æ¨¡å‹æ¨ç†
                with torch.no_grad():
                    try:
                        with torch.amp.autocast(device_type=device.type, dtype=dtype):
                            predictions = model_instance(imgs)
                    except:
                        # Fallbackæ–¹æ¡ˆ
                        try:
                            if device.type == "cuda":
                                with torch.cuda.amp.autocast(dtype=dtype):
                                    predictions = model_instance(imgs)
                            else:
                                predictions = model_instance(imgs)
                        except:
                            # æœ€åçš„fallback
                            predictions = model_instance(imgs)
                    
                    # ä»predictionsä¸­æå–pose_enc
                    pose_enc = predictions["pose_enc"]
                    logger.info(f"VGGTVideoCameraNode: pose_enc shape: {pose_enc.shape}")
                            
                # è½¬æ¢ä¸ºå†…å¤–å‚çŸ©é˜µ
                extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, imgs.shape[-2:])
                
                # å»é™¤æ‰¹æ¬¡ç»´åº¦
                if len(extrinsic.shape) == 4:  # (1,N,3,4)
                    extrinsic = extrinsic[0]   # (N,3,4)
                if len(intrinsic.shape) == 4:  # (1,N,3,3)
                    intrinsic = intrinsic[0]   # (N,3,3)
                    
                extrinsic = extrinsic.cpu()
                intrinsic = intrinsic.cpu()
                
                logger.info(f"VGGTVideoCameraNode: Final matrix shapes - "
                          f"extrinsic: {extrinsic.shape}, intrinsic: {intrinsic.shape}")

            # ç”ŸæˆJSONè¾“å‡º
            intrinsics_json, poses_json = _matrices_to_json(intrinsic.numpy(), extrinsic.numpy())

            # ç”Ÿæˆè½¨è¿¹é¢„è§ˆå›¾
            traj_tensor = _create_traj_preview(extrinsic)

            logger.info("VGGTVideoCameraNode: Camera estimation completed successfully")
            return (intrinsics_json, traj_tensor, poses_json)

        except Exception as e:
            error_msg = f"VGGTä¼°è®¡é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            empty_img = torch.ones((1, 400, 400, 3), dtype=torch.float32) * 0.1
            error_json = json.dumps({"success": False, "error": error_msg}, ensure_ascii=False, indent=2)
            return (error_json, empty_img, error_json)

class VGGTSingleImageCameraNode:
    """VGGT å•å¼ å›¾ç‰‡ç›¸æœºå‚æ•°ä¼°è®¡èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "vggt_model": ("VVL_VGGT_MODEL", {
                    "tooltip": "æ¥è‡ªVVLVGGTLoaderçš„VGGTæ¨¡å‹å®ä¾‹ï¼ŒåŒ…å«å·²åŠ è½½çš„æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯"
                }),
                "image": ("IMAGE", {
                    "tooltip": "è¾“å…¥çš„å•å¼ å›¾ç‰‡ï¼Œæ ¼å¼ä¸º[B,H,W,C]çš„tensor"
                }),
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING")
    RETURN_NAMES = ("intrinsics_json", "trajectory_preview", "poses_json")
    FUNCTION = "estimate"
    CATEGORY = "ğŸ’ƒVVL/VGGT"

    def estimate(self, vggt_model: Dict, image: torch.Tensor):
        """æ‰§è¡Œç›¸æœºå‚æ•°ä¼°è®¡"""
        try:
            # æ£€æŸ¥VGGTå·¥å…·å‡½æ•°æ˜¯å¦å¯ç”¨
            if not VGGT_UTILS_AVAILABLE:
                raise RuntimeError(f"VGGT utils not available: {_VGGT_UTILS_IMPORT_ERROR}")
            
            # ä»æ¨¡å‹å­—å…¸ä¸­è·å–ä¿¡æ¯
            model_instance = vggt_model['model']
            device = vggt_model['device']
            model_name = vggt_model['model_name']
            
            logger.info(f"VGGTSingleImageCameraNode: Using {model_name} on {device}")
            
            # ç¡®å®šæ•°æ®ç±»å‹
            if device.type == "cuda":
                try:
                    # å°è¯•ä½¿ç”¨BFloat16ï¼Œå¦‚æœä¸æ”¯æŒåˆ™fallbackåˆ°Float16
                    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
                except:
                    dtype = torch.float16
            else:
                dtype = torch.float32

            # ç¡®ä¿è¾“å…¥å›¾åƒæ ¼å¼æ­£ç¡®
            if len(image.shape) != 4:
                raise ValueError(f"è¾“å…¥å›¾åƒç»´åº¦é”™è¯¯ï¼ŒæœŸæœ›[B,H,W,C]ï¼Œå¾—åˆ°{image.shape}")
            
            # å°†å›¾åƒç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            imgs = image.to(device)
            logger.info(f"VGGTSingleImageCameraNode: Input image shape: {imgs.shape}")

            imgs = preprocess_tensor_like_vggt(imgs, mode="crop")  # æˆ– "pad"

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                try:
                    with torch.amp.autocast(device_type=device.type, dtype=dtype):
                        predictions = model_instance(imgs)
                except:
                    # Fallbackæ–¹æ¡ˆ
                    try:
                        if device.type == "cuda":
                            with torch.cuda.amp.autocast(dtype=dtype):
                                predictions = model_instance(imgs)
                        else:
                            predictions = model_instance(imgs)
                    except:
                        # æœ€åçš„fallback
                        predictions = model_instance(imgs)
                
                # ä»predictionsä¸­æå–pose_enc
                pose_enc = predictions["pose_enc"]
                logger.info(f"VGGTSingleImageCameraNode: pose_enc shape: {pose_enc.shape}")
                        
            # è½¬æ¢ä¸ºå†…å¤–å‚çŸ©é˜µ
            extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, imgs.shape[-2:])
            
            # å»é™¤æ‰¹æ¬¡ç»´åº¦
            if len(extrinsic.shape) == 4:  # (1,N,3,4)
                extrinsic = extrinsic[0]   # (N,3,4)
            if len(intrinsic.shape) == 4:  # (1,N,3,3)
                intrinsic = intrinsic[0]   # (N,3,3)
                
            extrinsic = extrinsic.cpu()
            intrinsic = intrinsic.cpu()
            
            logger.info(f"VGGTSingleImageCameraNode: Final matrix shapes - "
                      f"extrinsic: {extrinsic.shape}, intrinsic: {intrinsic.shape}")

            # ç”ŸæˆJSONè¾“å‡º
            intrinsics_json, poses_json = _matrices_to_json(intrinsic.numpy(), extrinsic.numpy())

            # ç”Ÿæˆè½¨è¿¹é¢„è§ˆå›¾
            traj_tensor = _create_traj_preview(extrinsic)

            logger.info("VGGTSingleImageCameraNode: Camera estimation completed successfully")
            return (intrinsics_json, traj_tensor, poses_json)

        except Exception as e:
            error_msg = f"VGGTä¼°è®¡é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            
            # è¿”å›é”™è¯¯ç»“æœ
            empty_img = torch.ones((1, 400, 400, 3), dtype=torch.float32) * 0.1
            error_json = json.dumps({"success": False, "error": error_msg}, ensure_ascii=False, indent=2)
            return (error_json, empty_img, error_json)

class CalculateMaskCentersSimple3D:
    """è®¡ç®—maskä¸­å¿ƒç‚¹çš„3Dä¸–ç•Œåæ ‡"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "masks": ("MASK", {}),
                "depth_image": ("IMAGE", {}),
                "intrinsics_json": ("STRING", {}),
                "poses_json": ("STRING", {}),
            },
            "optional": {
                "view_id": ("INT", {
                    "default": 0, "min": 0, "max": 100,
                    "tooltip": "ä½¿ç”¨å“ªä¸ªè§†è§’çš„ç›¸æœºå‚æ•°"
                }),
                "min_depth": ("FLOAT", {
                    "default": 0.5, "min": 0.01, "max": 1000.0,
                    "tooltip": "æ·±åº¦å›¾çš„æœ€å°æ·±åº¦å€¼ï¼ˆç±³ï¼‰"
                }),
                "max_depth": ("FLOAT", {
                    "default": 50.0, "min": 0.1, "max": 1000.0,
                    "tooltip": "æ·±åº¦å›¾çš„æœ€å¤§æ·±åº¦å€¼ï¼ˆç±³ï¼‰"
                }),
            }
        }
    
    CATEGORY = "ğŸ’ƒVVL/VGGT"
    FUNCTION = "main"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("mask_centers_3d",)
    
    def calculate_depth(self, x_cord, y_cord, depth_npy):
        """åŒçº¿æ€§æ’å€¼è®¡ç®—æ·±åº¦å€¼"""
        if len(depth_npy.shape) > 2:
            if depth_npy.shape[2] == 1:
                depth_npy = depth_npy[:, :, 0]
            else:
                depth_npy = np.mean(depth_npy, axis=2)
        
        h, w = depth_npy.shape
        x0, y0 = int(np.floor(x_cord)), int(np.floor(y_cord))
        x1, y1 = min(x0 + 1, w - 1), min(y0 + 1, h - 1)
        
        wx = x_cord - x0
        wy = y_cord - y0
        
        top = depth_npy[y0, x0] * (1 - wx) + depth_npy[y0, x1] * wx
        bottom = depth_npy[y1, x0] * (1 - wx) + depth_npy[y1, x1] * wx
        
        return float(top * (1 - wy) + bottom * wy)
    
    def main(self, masks, depth_image, intrinsics_json, poses_json, 
             view_id=0, min_depth=0.5, max_depth=50.0): 
        try:
            # è§£æç›¸æœºå‚æ•°
            intrinsics_data = json.loads(intrinsics_json)
            poses_data = json.loads(poses_json)
            
            # è·å–æŒ‡å®šè§†è§’çš„ç›¸æœºå‚æ•°
            intrinsic_matrix = None
            extrinsic_matrix = None
            
            for camera in intrinsics_data["cameras"]:
                if camera["view_id"] == view_id:
                    intrinsic_matrix = np.array(camera["intrinsic_matrix"])
                    break
            
            for pose in poses_data["poses"]:
                if pose["view_id"] == view_id:
                    extrinsic_matrix = np.array(pose["extrinsic_matrix"])
                    break
            
            if intrinsic_matrix is None or extrinsic_matrix is None:
                raise ValueError(f"æ‰¾ä¸åˆ°view_id={view_id}çš„ç›¸æœºå‚æ•°")
            
            # è½¬æ¢æ·±åº¦å›¾ä¸ºnumpyæ•°ç»„
            depth_np = depth_image[0].cpu().numpy()
            
            # ç›¸æœºå‚æ•°
            fx, fy = intrinsic_matrix[0, 0], intrinsic_matrix[1, 1]
            cx, cy = intrinsic_matrix[0, 2], intrinsic_matrix[1, 2]
            R = extrinsic_matrix[:3, :3]
            t = extrinsic_matrix[:3, 3]
            R_inv = R.T
            t_world = -R_inv @ t
            
            # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
            mask_centers_3d = []
            
            # éå†æ¯ä¸ªmask
            for i in range(masks.shape[0]):
                mask = masks[i].cpu().numpy()
                
                # æ‰¾åˆ°maskä¸­æ‰€æœ‰éé›¶ç‚¹çš„åæ ‡
                y_coords, x_coords = np.where(mask > 0)
                
                if len(y_coords) > 0:
                    # è®¡ç®—maskçš„ä¸­å¿ƒç‚¹
                    center_y = np.mean(y_coords)
                    center_x = np.mean(x_coords)
                    
                    # è®¡ç®—ä¸­å¿ƒç‚¹çš„æ·±åº¦å€¼
                    center_depth_01 = self.calculate_depth(center_x, center_y, depth_np)
                    depth_absolute = center_depth_01 * (max_depth - min_depth) + min_depth
                    
                    # è½¬æ¢ä¸º3Dä¸–ç•Œåæ ‡
                    x_cam = (center_x - cx) * depth_absolute / fx
                    y_cam = (center_y - cy) * depth_absolute / fy
                    z_cam = depth_absolute
                    
                    cam_coords = np.array([x_cam, y_cam, z_cam])
                    world_coords = R_inv @ cam_coords + t_world
                    
                    # å°†3Dåæ ‡æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    mask_centers_3d.append([
                        float(world_coords[0]),  # world_x
                        float(world_coords[1]),  # world_y
                        float(world_coords[2])   # world_z
                    ])
            
            logger.info(f"CalculateMaskCentersSimple3D: å¤„ç†äº† {len(mask_centers_3d)} ä¸ªmask")
            return (json.dumps(mask_centers_3d, ensure_ascii=False),)
        
        except Exception as e:
            error_msg = f"Mask 3Dåæ ‡è®¡ç®—é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return (json.dumps({"error": error_msg}, ensure_ascii=False),)

class VGGTToBlenderCameraNode:
    """å°†VGGTç›¸æœºå‚æ•°è½¬æ¢ä¸ºBlenderå¯ç”¨æ ¼å¼çš„èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "intrinsics_json": ("STRING", {
                    "tooltip": "æ¥è‡ªVGGTèŠ‚ç‚¹çš„ç›¸æœºå†…å‚JSONæ•°æ®"
                }),
                "poses_json": ("STRING", {
                    "tooltip": "æ¥è‡ªVGGTèŠ‚ç‚¹çš„ç›¸æœºå¤–å‚JSONæ•°æ®"
                }),
                "view_id": ("INT", {
                    "default": 0, "min": 0, "max": 100,
                    "tooltip": "é€‰æ‹©è¦è½¬æ¢çš„è§†è§’ID"
                }),
                "image_width": ("INT", {
                    "default": 1920, "min": 1, "max": 8192,
                    "tooltip": "åŸå§‹å›¾åƒçš„å®½åº¦ï¼ˆåƒç´ ï¼‰"
                }),
                "image_height": ("INT", {
                    "default": 1080, "min": 1, "max": 8192,
                    "tooltip": "åŸå§‹å›¾åƒçš„é«˜åº¦ï¼ˆåƒç´ ï¼‰"
                }),
                "sensor_width": ("FLOAT", {
                    "default": 36.0, "min": 1.0, "max": 100.0,
                    "tooltip": "ä¼ æ„Ÿå™¨å®½åº¦ï¼ˆæ¯«ç±³ï¼‰ï¼Œå…¨ç”»å¹…ä¸º36mm"
                }),
            },
            "optional": {
                "coordinate_system": (["OpenCV", "Blender"], {
                    "default": "Blender",
                    "tooltip": "è¾“å‡ºåæ ‡ç³»ç±»å‹"
                }),
                "scale_factor": ("FLOAT", {
                    "default": 1.0, "min": 0.001, "max": 1000.0,
                    "tooltip": "åæ ‡ç¼©æ”¾å› å­"
                }),
            }
        }
    
    CATEGORY = "ğŸ’ƒVVL/VGGT"
    FUNCTION = "convert_to_blender"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("blender_camera_data",)
    
    def _rotation_matrix_to_euler(self, R):
        """å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºæ¬§æ‹‰è§’ï¼ˆZYXé¡ºåºï¼‰"""
        # æå–æ¬§æ‹‰è§’ï¼ˆZYXé¡ºåºï¼Œå¯¹åº”Blenderçš„é»˜è®¤æ—‹è½¬é¡ºåºï¼‰
        sy = math.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])
        
        singular = sy < 1e-6
        
        if not singular:
            x = math.atan2(R[2, 1], R[2, 2])
            y = math.atan2(-R[2, 0], sy)
            z = math.atan2(R[1, 0], R[0, 0])
        else:
            x = math.atan2(-R[1, 2], R[1, 1])
            y = math.atan2(-R[2, 0], sy)
            z = 0
        
        return [x, y, z]  # å¼§åº¦åˆ¶
    
    def _convert_coordinate_system(self, position, rotation_matrix, coord_system):
        """è½¬æ¢åæ ‡ç³»"""
        if coord_system == "Blender":
            # OpenCVåˆ°Blenderçš„åæ ‡ç³»è½¬æ¢
            # OpenCV: +Xå³, +Yä¸‹, +Zå‰
            # Blender: +Xå³, +Yå‰, +Zä¸Š
            
            # åæ ‡ç³»è½¬æ¢çŸ©é˜µ
            coord_transform = np.array([
                [1,  0,  0],
                [0,  0,  1],
                [0, -1,  0]
            ])
            
            # è½¬æ¢ä½ç½®
            new_position = coord_transform @ position
            
            # è½¬æ¢æ—‹è½¬çŸ©é˜µ
            new_rotation = coord_transform @ rotation_matrix @ coord_transform.T
            
            return new_position, new_rotation
        else:
            # ä¿æŒOpenCVåæ ‡ç³»
            return position, rotation_matrix
    
    def convert_to_blender(self, intrinsics_json, poses_json, view_id, 
                          image_width, image_height, sensor_width, 
                          coordinate_system="Blender", scale_factor=1.0):
        """è½¬æ¢VGGTç›¸æœºå‚æ•°ä¸ºBlenderæ ¼å¼"""
        try:
            # è§£æJSONæ•°æ®
            intrinsics_data = json.loads(intrinsics_json)
            poses_data = json.loads(poses_json)
            
            # æŸ¥æ‰¾æŒ‡å®šè§†è§’çš„æ•°æ®
            intrinsic_matrix = None
            extrinsic_matrix = None
            camera_position = None
            
            for camera in intrinsics_data["cameras"]:
                if camera["view_id"] == view_id:
                    intrinsic_matrix = np.array(camera["intrinsic_matrix"])
                    break
            
            for pose in poses_data["poses"]:
                if pose["view_id"] == view_id:
                    extrinsic_matrix = np.array(pose["extrinsic_matrix"])
                    camera_position = np.array(pose["position"])
                    break
            
            if intrinsic_matrix is None or extrinsic_matrix is None:
                raise ValueError(f"æ‰¾ä¸åˆ°view_id={view_id}çš„ç›¸æœºæ•°æ®")
            
            # æå–ç›¸æœºå‚æ•°
            fx = intrinsic_matrix[0, 0]
            fy = intrinsic_matrix[1, 1]
            cx = intrinsic_matrix[0, 2]
            cy = intrinsic_matrix[1, 2]
            
            # æå–æ—‹è½¬çŸ©é˜µ
            R = extrinsic_matrix[:3, :3]
            
            # è®¡ç®—ç„¦è·ï¼ˆæ¯«ç±³ï¼‰
            focal_length_mm = fx * sensor_width / image_width
            
            # åº”ç”¨ç¼©æ”¾å› å­
            camera_position = camera_position * scale_factor
            
            # åæ ‡ç³»è½¬æ¢
            converted_position, converted_rotation = self._convert_coordinate_system(
                camera_position, R, coordinate_system
            )
            
            # è½¬æ¢ä¸ºæ¬§æ‹‰è§’
            euler_angles = self._rotation_matrix_to_euler(converted_rotation)
            euler_degrees = [math.degrees(angle) for angle in euler_angles]
            
            # ç”ŸæˆBlenderç›¸æœºæ•°æ®
            blender_camera_data = {
                "view_id": view_id,
                "coordinate_system": coordinate_system,
                "camera_settings": {
                    "location": {
                        "x": float(converted_position[0]),
                        "y": float(converted_position[1]),
                        "z": float(converted_position[2])
                    },
                    "rotation_euler": {
                        "x": float(euler_angles[0]),  # å¼§åº¦
                        "y": float(euler_angles[1]),
                        "z": float(euler_angles[2])
                    },
                    "rotation_degrees": {
                        "x": float(euler_degrees[0]),  # åº¦æ•°
                        "y": float(euler_degrees[1]),
                        "z": float(euler_degrees[2])
                    },
                    "lens": float(focal_length_mm),
                    "sensor_width": float(sensor_width),
                    "sensor_fit": "HORIZONTAL"
                },
                "original_parameters": {
                    "fx": float(fx),
                    "fy": float(fy),
                    "cx": float(cx),
                    "cy": float(cy),
                    "image_width": image_width,
                    "image_height": image_height,
                    "scale_factor": scale_factor
                }
            }
            
            logger.info(f"VGGTToBlenderCameraNode: æˆåŠŸè½¬æ¢view_id={view_id}çš„ç›¸æœºå‚æ•°")
            
            return (
                json.dumps(blender_camera_data, ensure_ascii=False, indent=2)
            )
            
        except Exception as e:
            error_msg = f"VGGTåˆ°Blenderè½¬æ¢é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            error_json = json.dumps({"error": error_msg}, ensure_ascii=False, indent=2)
            return (error_json,)

# -----------------------------------------------------------------------------
# èŠ‚ç‚¹æ³¨å†Œ
# -----------------------------------------------------------------------------

NODE_CLASS_MAPPINGS = {
    "VGGTVideoCameraNode": VGGTVideoCameraNode,
    "VGGTSingleImageCameraNode": VGGTSingleImageCameraNode,
    "CalculateMaskCentersSimple3D": CalculateMaskCentersSimple3D,
    "VGGTToBlenderCameraNode": VGGTToBlenderCameraNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VGGTVideoCameraNode": "VVL VGGT Video Camera Estimator",
    "VGGTSingleImageCameraNode": "VVL VGGT Single Image Camera Estimator",
    "CalculateMaskCentersSimple3D": "VVL Mask Centers 3D Calculator",
    "VGGTToBlenderCameraNode": "VVL VGGT to Blender Camera Converter"
}

# å¦‚æœæ¨¡å‹åŠ è½½å™¨å¯ç”¨ï¼Œæ·»åŠ åˆ°æ˜ å°„ä¸­
if MODEL_LOADER_AVAILABLE:
    NODE_CLASS_MAPPINGS["VVLVGGTLoader"] = VVLVGGTLoader
    NODE_DISPLAY_NAME_MAPPINGS["VVLVGGTLoader"] = "VVL VGGT Model Loader" 